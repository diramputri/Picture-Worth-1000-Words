---
title: "Information Gain and Entropy Notes"
author: "Andira Putri"
output: pdf_document
---

The old pre-med, chemistry major in me thought that **entropy** was a term only used in the physical sciences as a measure of disorder. However, one of my colleagues brought entropy up as a way of finding the most interesting features in a very high-dimensional data set...so I knew I had to study this further in the context of data science...

## What is Information Theory?

In the past, scientists needed a way to quantify information. For example:

* Naman likes superhero movies.

* Naman likes Marvel superhero movies, especially with Iron Man.

Clearly, the second statement contains more information than the very general first statement. Scientists wondered how the differences in the statements could be quantified, and they questioned how to represent the distinctions in a mathematical expression. Claude Shannon came up with the notion of entropy, which marked the beginning of the Digital Information Age. Shannon believed the "semantic aspects of data are irrelevant." In other words, the nature and meaning of the data don't really matter compared to the information content. This gave rise to Information Theory, which paved paths for analytics and artificial intelligence.

Now, think about a scenario where you are trying to guess the author of a book given the text. Words like "it" or "the" would not be very helpful since they are commonly used. Pronouns like "it" or "the" contain less information about the author than rare words, like "infinities" or "fathom". Claude Shannon quantified information in terms of probability distributions and uncertainty, so let's take that approach. With that being said, we now express our scenario mathematically.

### Mathemetical Approach

Let $x$ be a random variable for all possible words. $p(x)$ is the probability of seeing word $x$. Finally, $h(x)$ is the information gained from seeing word $x$. If we had another word $y$, we could also find the information gained from seeing $x$ and $y$ together, i.e. $h(x,y)$. Our goal is to find a suitable functtion $h$.

First, we discuss independence between words $x$ and $y$. Two words, say "furry" and "cat", are not independent. Seeing "furry" makes seeing "cat" more likely. Therefore, there is a small information gain from seeing "cat" after "furry". However, when two words are independent, $h(x,y) = h(x) + h(y)$, which is the sum of the information gained from spotting the two words alone.

We have that $p(x,y) = p(x)p(y)$, which is the joint probability of independent observations. We want our formula for h to be related to p. However, we have that the joint probability $p(x,y)$ is a product, and joint information $h(x,y)$ is a sum. We use the product rule of logarithms to convert the joint probability product into a sum.

$log[p(x, y)] = log[p(x)p(y)] = log p(x) + log p(y)$

The base of the log can be anything, but the most common value is 2. Now that we have the above expression, we have that the information gained from seeing $x$ is $h(x) = -logp(x)$. Therefore, the information gain from seeing both $x$ and $y$ is:

$h(x,y) = -logp(x,y) = -logp(x) + [-logp(y)] = h(x) + h(y)$.

## What is Entropy?---
title: "Information Gain and Entropy Notes"
author: "Andira Putri"
output: pdf_document
---

The old pre-med, chemistry major in me thought that **entropy** was a term only used in the physical sciences as a measure of disorder. However, one of my colleagues brought entropy up as a way of finding the most interesting features in a very high-dimensional data set...so I knew I had to study this further in the context of data science...

## What is Information Theory?

In the past, scientists needed a way to quantify information. For example:

* Naman likes superhero movies.

* Naman likes Marvel superhero movies, especially with Iron Man.

Clearly, the second statement contains more information than the very general first statement. Scientists wondered how the differences in the statements could be quantified, and they questioned how to represent the distinctions in a mathematical expression. Claude Shannon came up with the notion of entropy, which marked the beginning of the Digital Information Age. Shannon believed the “semantic aspects of data are irrelevant.” In other words, the nature and meaning of the data don't really matter compared to the information content. This gave rise to Information Theory, which paved paths for analytics and artificial intelligence.

Now, think about a scenario where you are trying to guess the author of a book given the text. Words like "it" or "the" would not be very helpful since they are commonly used. Pronouns like "it" or "the" contain less information about the author than rare words, like "infinities" or "constellations". Claude Shannon quantified information in terms of probability distributions and uncertainty, so let's take that approach. With that being said, we now express our scenario mathematically.

### Mathemetical Approach

Let $x$ be a random variable for all possible words. $p(x)$ is the probability of seeing word $x$. Finally, $h(x)$ is the information gained from seeing word $x$. If we had another word $y$, we could also find the information gained from seeing $x$ and $y$ together, i.e. $h(x,y)$. Our goal is to find a suitable functtion $h$.

First, we discuss independence between words $x$ and $y$. Two words, say "furry" and "cat", are not independent. Seeing "furry" makes seeing "cat" more likely. Therefore, there is a small information gain from seeing "cat" after "furry". However, when two words are independent, $h(x,y) = h(x) + h(y)$, which is the sum of the information gained from spotting the two words alone.

We have that $p(x,y) = p(x)p(y)$, which is the joint probability of independent observations. We want our formula for h to be related to p. However, we have that the joint probability $p(x,y)$ is a product, and joint information $h(x,y)$ is a sum. We use the product rule of logarithms to convert the joint probability product into a sum.

$log[p(x, y)] = log[p(x)p(y)] = log p(x) + log p(y)$

The base of the log can be anything, but the most common value is 2. Now that we have the above expression, we have that the information gained from seeing $x$ is $h(x) = -logp(x)$. Therefore, the information gain from seeing both $x$ and $y$ is:

$h(x,y) = -logp(x,y) = -logp(x) + [-logp(y)] = h(x) + h(y)$.

## What is Entropy?

The expectation of our information gain function $h$ gives us entropy, denoted as $H$:

$\mathbf{H}_{x}=-\sum_{x} p(x) \log p(x)$.

With the definition of expectation in mind, our entropy $H_x$ is the average information gained by observing a randomly-sampled $x$. Entropy can help data scientists find feature representations by answering: How can we represent the data with as few features as possible while still keeping sufficient information to make accurate predictions? This avoids high dimensionality and overfitting.

### Cross-Entropy

## Project

We compare the entropies of a book excerpt and image. The book I chose is "The Fault in Our Stars" by John Green. The image is a still from "All of the Stars" by Ed Sheeran, which is the main song of the movie adaptation of the chosen book.
