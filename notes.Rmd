---
title: "Information Gain and Entropy Notes"
author: "Andira Putri"
output: pdf_document
---

The old pre-med, chemistry major in me thought that **entropy** was a term only used in the physical sciences as a measure of disorder. However, one of my colleagues brought entropy up as a way of finding the most interesting features in a very high-dimensional data set...so I knew I had to study this further in the context of data science...

## What is Information Theory?

In the past, scientists needed a way to quantify information. For example:

* Naman likes superhero movies.

* Naman likes Marvel superhero movies, especially with Iron Man.

Clearly, the second statement contains more information than the very general first statement. Scientists wondered how the differences in the statements could be quantified, and they questioned how to represent the distinctions in a mathematical expression. Claude Shannon came up with the notion of entropy, which marked the beginning of the Digital Information Age. Shannon believed the "semantic aspects of data are irrelevant." In other words, the nature and meaning of the data don't really matter compared to the information content. This gave rise to Information Theory, which paved paths for analytics and artificial intelligence.

Now, think about a scenario where you are trying to guess the author of a book given the text. Words like "it" or "the" would not be very helpful since they are commonly used. Pronouns like "it" or "the" contain less information about the author than rare words, like "infinities" or "constellations". Claude Shannon quantified information in terms of probability distributions and uncertainty, so let's take that approach. With that being said, we now express our scenario mathematically.

### Mathemetical Approach

Let $x$ be a random variable for all possible words. $p(x)$ is the probability of seeing word $x$. Finally, $h(x)$ is the information gained from seeing word $x$. If we had another word $y$, we could also find the information gained from seeing $x$ and $y$ together, i.e. $h(x,y)$. Our goal is to find a suitable functtion $h$.

First, we discuss independence between words $x$ and $y$. Two words, say "furry" and "cat", are not independent. Seeing "furry" makes seeing "cat" more likely. Therefore, there is a small information gain from seeing "cat" after "furry". However, when two words are independent, $h(x,y) = h(x) + h(y)$, which is the sum of the information gained from spotting the two words alone.

We have that $p(x,y) = p(x)p(y)$, which is the joint probability of independent observations. We want our formula for h to be related to p. However, we have that the joint probability $p(x,y)$ is a product, and joint information $h(x,y)$ is a sum. We use the product rule of logarithms to convert the joint probability product into a sum.

$log[p(x, y)] = log[p(x)p(y)] = log p(x) + log p(y)$

The base of the log can be anything, but the most common value is 2. Now that we have the above expression, we have that the information gained from seeing $x$ is $h(x) = -logp(x)$. Therefore, the information gain from seeing both $x$ and $y$ is:

$h(x,y) = -logp(x,y) = -logp(x) + [-logp(y)] = h(x) + h(y)$.

## What is Entropy?

The expectation of our information gain function $h$ gives us entropy, denoted as $H$:

$\mathbf{H}_{x}=-\sum_{x} p(x) \log p(x)$.

With the definition of expectation in mind, our entropy $H_x$ is the average information gained by observing a randomly-sampled $x$. Entropy can help data scientists find feature representations by answering: How can we represent the data with as few features as possible while still keeping sufficient information to make accurate predictions? This avoids high dimensionality and overfitting.

### Cross-Entropy

We revisit the author problem - identifying the author of a book by the words used. Our problem is essentially a classification problem, where one group is {text from Author A}, and the other group is {text not from Author A}. The order of the words does not matter in both groups, just the unique elements. Each book is a *vector* **x** where each entry $j$ represents a word. $j=1$ if a word is in the book, $j=0$ if the word is not in the book.

We assume there is a probability distribution $p(x)$ for a book $x$ to be written by Author A. If we know this ground truth probability, we can predict if a book was written by Author A if $p(x) > 0.5$, or any threshold of choice. Unfortunately, we don't have that ground truth. Instead, we write a model $q$. We find parameters for $q$ to approximate $p$. Then, we use $q$ to see if a book truly is written by Author A or not. Recall that $p$ is related to $h$ by $h(x) = -logp(x)$. Now, we relate our model $q(x)$ to the information gain by $h(x) = logq(x)$. However, if $x$ is distributed by $p$, then the entropy (expectation of information) is actually:

$\mathbf{H}(p, q)=-\sum_{x} p(x) \log q(x)$

This is the cross entropy of a model $q$ with respect to $p$, the true distribution. Like how one would utilize gradient boosting, we can think of the cross entropy as a cost function. We want to find $q$ such that the cross entropy is minimized. The best solution occurs when $p$ is very close to $q$.

## Project

We compare the entropies of a book summary and image. The book I chose is "The Fault in Our Stars" by John Green. The image is a still from "All of the Stars" by Ed Sheeran, which is the main song of the book's movie adaptation.

![Still from "All of the Stars" by Ed Sheeran](allofthestars.png)